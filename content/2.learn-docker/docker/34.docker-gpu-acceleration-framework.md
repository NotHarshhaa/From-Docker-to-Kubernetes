---
title: Docker GPU Acceleration Framework
description: Comprehensive guide to Docker's GPU acceleration capabilities for high-performance computing, AI/ML workloads, and multi-vendor hardware support
icon: 'lucide:cpu'
authors:
  - name: H A R S H H A A
    username: NotHarshhaa
    avatar: https://www.github.com/NotHarshhaa.png
    to: https://github.com/NotHarshhaa
    target: _blank
navBadges:
  - value: New
    type: lime
---

## Introduction to Docker GPU Acceleration Framework

::alert{type="info"}
Docker's GPU Acceleration Framework provides a comprehensive ecosystem for integrating and leveraging GPU hardware within containerized environments.
::

::div{class="mt-6 grid grid-cols-1 md:grid-cols-2 gap-4"}
  ::div{class="bg-blue-50 dark:bg-blue-900/20 rounded-xl p-5"}
    ::div{class="flex items-center"}
      ::div{class="text-blue-500 dark:text-blue-400"}
        <Icon name="lucide:cpu" class="h-5 w-5" />
      ::
      ::div{class="ml-2 font-semibold"}
        Hardware Acceleration
      ::
    ::
    ::div{class="mt-2 text-sm"}
      Direct access to physical GPU resources for performance-critical workloads
    ::
  ::
  ::div{class="bg-blue-50 dark:bg-blue-900/20 rounded-xl p-5"}
    ::div{class="flex items-center"}
      ::div{class="text-blue-500 dark:text-blue-400"}
        <Icon name="lucide:layers" class="h-5 w-5" />
      ::
      ::div{class="ml-2 font-semibold"}
        Vendor-Agnostic Support
      ::
    ::
    ::div{class="mt-2 text-sm"}
      Unified interface for NVIDIA, AMD, and Intel GPU technologies
    ::
  ::
  ::div{class="bg-blue-50 dark:bg-blue-900/20 rounded-xl p-5"}
    ::div{class="flex items-center"}
      ::div{class="text-blue-500 dark:text-blue-400"}
        <Icon name="lucide:gauge" class="h-5 w-5" />
      ::
      ::div{class="ml-2 font-semibold"}
        Resource Optimization
      ::
    ::
    ::div{class="mt-2 text-sm"}
      Fine-grained allocation and monitoring of GPU resources
    ::
  ::
  ::div{class="bg-blue-50 dark:bg-blue-900/20 rounded-xl p-5"}
    ::div{class="flex items-center"}
      ::div{class="text-blue-500 dark:text-blue-400"}
        <Icon name="lucide:brain" class="h-5 w-5" />
      ::
      ::div{class="ml-2 font-semibold"}
        AI/ML Enablement
      ::
    ::
    ::div{class="mt-2 text-sm"}
      Streamlined deployment of compute-intensive machine learning workflows
    ::
  ::
::

This guide explores the architecture, configuration options, and best practices for implementing GPU-accelerated containers across development and production environments, enabling high-performance computing workloads within the Docker ecosystem.

## GPU Acceleration Architecture

### Core Components

::div{class="mb-6"}
  Docker's GPU Acceleration Framework consists of several interdependent components that enable container access to GPU hardware:
::

::div{class="pl-4 border-l-4 border-blue-500 dark:border-blue-400 mb-6"}
  ::ul{class="space-y-2"}
    ::li{class="flex items-start"}
      ::div{class="text-blue-600 dark:text-blue-400 mr-2 mt-1"}
        <Icon name="lucide:box" class="h-5 w-5" />
      ::
      ::div
        **Container Runtime Extensions**: Docker Engine components that facilitate GPU device access
      ::
    ::
    ::li{class="flex items-start"}
      ::div{class="text-blue-600 dark:text-blue-400 mr-2 mt-1"}
        <Icon name="lucide:cpu" class="h-5 w-5" />
      ::
      ::div
        **Driver Integration Layer**: Vendor-specific modules that interface with GPU drivers
      ::
    ::
    ::li{class="flex items-start"}
      ::div{class="text-blue-600 dark:text-blue-400 mr-2 mt-1"}
        <Icon name="lucide:bar-chart" class="h-5 w-5" />
      ::
      ::div
        **Resource Management System**: Allocation and monitoring of GPU memory and compute units
      ::
    ::
    ::li{class="flex items-start"}
      ::div{class="text-blue-600 dark:text-blue-400 mr-2 mt-1"}
        <Icon name="lucide:scan" class="h-5 w-5" />
      ::
      ::div
        **Hardware Discovery**: Automatic detection and characterization of available GPU resources
      ::
    ::
    ::li{class="flex items-start"}
      ::div{class="text-blue-600 dark:text-blue-400 mr-2 mt-1"}
        <Icon name="lucide:layers" class="h-5 w-5" />
      ::
      ::div
        **Orchestration Interface**: Integration points for multi-container GPU workload management
      ::
    ::
  ::
::

```
┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│                    Docker Container                             │
│                                                                 │
│   ┌────────────────────────┐      ┌────────────────────────┐   │
│   │                        │      │                        │   │
│   │   Application Code     │      │     GPU Libraries      │   │
│   │                        │      │    (CUDA, ROCm, etc)   │   │
│   └────────────┬───────────┘      └────────────┬───────────┘   │
│                │                                │               │
│                └────────────┬─────────────────┬┘               │
│                             │                 │                 │
└─────────────────────────────┼─────────────────┼─────────────────┘
                              │                 │
┌─────────────────────────────┼─────────────────┼─────────────────┐
│                             │                 │                 │
│   ┌─────────────────────────▼─────────────────▼───────────┐     │
│   │                                                       │     │
│   │             Docker GPU Acceleration Layer            │     │
│   │                                                       │     │
│   └─────────────────────────┬───────────────────────────┬─┘     │
│                             │                           │       │
│   ┌─────────────────────────▼───────┐   ┌───────────────▼───┐   │
│   │                               │   │                   │   │
│   │  Vendor-Specific GPU Drivers  │   │  Resource Monitor │   │
│   │                               │   │                   │   │
│   └───────────────┬───────────────┘   └───────────────────┘   │
│                   │                                           │
│                   ▼                                           │
│   ┌─────────────────────────────────────────────────────┐     │
│   │                                                     │     │
│   │                   GPU Hardware                      │     │
│   │                                                     │     │
│   └─────────────────────────────────────────────────────┘     │
│                                                               │
│                        Host System                            │
└───────────────────────────────────────────────────────────────┘
```

### Multi-Vendor Support Architecture

::div{class="mb-4"}
  Docker's GPU framework provides unified interfaces across major GPU vendors:
::

::div{class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-6"}
  ::div{class="bg-green-50 dark:bg-green-900/20 p-4 rounded-lg border border-green-100 dark:border-green-800"}
    ::div{class="flex items-center mb-2"}
      ::div{class="p-2 bg-green-100 dark:bg-green-800 rounded-md"}
        <Icon name="logos:nvidia" class="h-5 w-5" />
      ::
      ::div{class="ml-2 font-bold text-green-800 dark:text-green-400"}
        NVIDIA
      ::
    ::
    ::div{class="text-sm"}
      Full CUDA ecosystem support via nvidia-container-toolkit
    ::
  ::
  ::div{class="bg-red-50 dark:bg-red-900/20 p-4 rounded-lg border border-red-100 dark:border-red-800"}
    ::div{class="flex items-center mb-2"}
      ::div{class="p-2 bg-red-100 dark:bg-red-800 rounded-md"}
        <Icon name="logos:amd" class="h-5 w-5" />
      ::
      ::div{class="ml-2 font-bold text-red-800 dark:text-red-400"}
        AMD
      ::
    ::
    ::div{class="text-sm"}
      ROCm platform integration through device pass-through
    ::
  ::
  ::div{class="bg-blue-50 dark:bg-blue-900/20 p-4 rounded-lg border border-blue-100 dark:border-blue-800"}
    ::div{class="flex items-center mb-2"}
      ::div{class="p-2 bg-blue-100 dark:bg-blue-800 rounded-md"}
        <Icon name="logos:intel" class="h-5 w-5" />
      ::
      ::div{class="ml-2 font-bold text-blue-800 dark:text-blue-400"}
        Intel
      ::
    ::
    ::div{class="text-sm"}
      oneAPI acceleration via Intel GPU runtime plugins
    ::
  ::
  ::div{class="bg-purple-50 dark:bg-purple-900/20 p-4 rounded-lg border border-purple-100 dark:border-purple-800"}
    ::div{class="flex items-center mb-2"}
      ::div{class="p-2 bg-purple-100 dark:bg-purple-800 rounded-md"}
        <Icon name="lucide:combine" class="h-5 w-5" />
      ::
      ::div{class="ml-2 font-bold text-purple-800 dark:text-purple-400"}
        Multi-vendor
      ::
    ::
    ::div{class="text-sm"}
      Simultaneous support for heterogeneous GPU hardware
    ::
  ::
::

## Getting Started with GPU Acceleration

### Prerequisites

Before enabling GPU acceleration in Docker, ensure your system meets these requirements:

1. **GPU Hardware**: Compatible NVIDIA, AMD, or Intel GPU
2. **Updated Drivers**: Latest vendor-specific GPU drivers installed on the host
3. **Docker Engine**: Version 19.03 or newer with GPU support
4. **Vendor Toolkit**: Appropriate vendor-specific container toolkit installed

```bash
# Check for available GPU hardware
lspci | grep -i 'vga\|3d\|display'

# Verify NVIDIA driver installation
nvidia-smi

# Check AMD GPU detection
rocm-smi

# Verify Intel GPU is recognized
intel_gpu_top
```

### Installing GPU Support

#### NVIDIA GPU Setup

Install the NVIDIA Container Toolkit:

```bash
# Add NVIDIA package repository
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

# Install NVIDIA runtime
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

#### AMD GPU Setup

Configure Docker for AMD GPU access:

```bash
# Install ROCm platform
sudo apt update
sudo apt install rocm-dev

# Configure Docker for device pass-through
sudo usermod -a -G video $USER
sudo usermod -a -G render $USER
```

#### Intel GPU Setup

Set up Intel GPU acceleration:

```bash
# Install Intel compute runtime
sudo apt-get update
sudo apt-get install -y intel-opencl-icd intel-level-zero-gpu level-zero

# Configure Docker permissions
sudo usermod -a -G render $USER
sudo usermod -a -G video $USER
```

### Basic GPU Container Usage

Run a container with GPU access using the `--gpus` flag:

```bash
# Run NVIDIA GPU-enabled container
docker run --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi

# Run with specific GPU devices
docker run --gpus device=0,1 nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi

# Run with GPU memory limit (NVIDIA only)
docker run --gpus 'device=0,capabilities=utility,compute,memory:2GB' nvidia/cuda samples
```

## Advanced GPU Configuration

::callout{icon="lucide:zap" type="warning" class="mb-4"}
#summary
GPU Resource Allocation
#content
Control how containers access GPU resources with fine-grained allocation strategies.
::

::code-group
```bash [Specific GPU Selection]
# Allocate specific GPU devices by index
docker run --gpus 'device=1,2' tensorflow/tensorflow:latest-gpu python -c 'import tensorflow as tf; print(tf.config.list_physical_devices("GPU"))'
```

```bash [Environment Variable Control]
# Limit visible GPU devices with environment variables (framework-specific)
docker run -e CUDA_VISIBLE_DEVICES=0,1 tensorflow/tensorflow:latest-gpu python -c 'import tensorflow as tf; print(tf.config.list_physical_devices("GPU"))'
```

```bash [Memory Limitations]
# Set compute mode and memory limits (NVIDIA)
docker run --gpus 'all,capabilities=compute,utility,graphics,video,display,memory:6GB' nvidia/cuda:latest nvidia-smi
```
::

### Multi-GPU Orchestration

Configure containers to efficiently utilize multiple GPUs:

```bash
# Distribute workload across all available GPUs
docker run --gpus all -e NVIDIA_VISIBLE_DEVICES=all pytorch/pytorch:latest python -c 'import torch; print(f"Available GPUs: {torch.cuda.device_count()}")'

# Custom device mapping for multi-node training
docker run --gpus 'device=0,1,2,3' -e MASTER_ADDR=localhost -e MASTER_PORT=8888 -e WORLD_SIZE=4 pytorch/pytorch:latest python distributed_training.py
```

### Monitoring GPU Usage

Track GPU utilization and performance within containers:

```bash
# Monitor GPU utilization in real-time
docker run --gpus all --rm nvidia/cuda:latest watch -n 0.5 nvidia-smi

# Track GPU memory usage with detailed metrics
docker run --gpus all --rm nvidia/cuda:latest nvidia-smi --query-gpu=timestamp,name,utilization.gpu,utilization.memory,memory.total,memory.used --format=csv -l 1
```

## Docker Compose with GPU Support

### Compose File Configuration

Enable GPU resources in Docker Compose services:

```yaml
version: '3'
services:
  ml-training:
    image: tensorflow/tensorflow:latest-gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu, utility, compute]
    volumes:
      - ./training_data:/data
      - ./models:/models
    command: python /models/train.py --data-dir /data
    
  inference-api:
    image: custom/inference-service:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8501:8501"
    depends_on:
      - ml-training
```

### GPU Resource Limits in Compose

Set resource constraints to prevent GPU memory contention:

```yaml
version: '3'
services:
  gpu-service-1:
    image: tensorflow/tensorflow:latest-gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu, utility]
    environment:
      - TF_MEMORY_ALLOCATION=0.5  # Utilize 50% of GPU memory
      
  gpu-service-2:
    image: pytorch/pytorch:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu, utility]
    environment:
      - CUDA_MEM_ALLOC_CONF=max_split_size_mb:256  # Limit memory block allocation
```

## GPU-Accelerated Use Cases

### AI/ML Training Workloads

Configure containers for efficient machine learning model training:

```bash
# Run distributed TensorFlow training
docker run --gpus all -v $(pwd)/data:/data -v $(pwd)/models:/models tensorflow/tensorflow:latest-gpu \
  python -c "import tensorflow as tf; strategy = tf.distribute.MirroredStrategy(); print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"

# PyTorch distributed training container
docker run --gpus all -v $(pwd)/data:/data -v $(pwd)/models:/models pytorch/pytorch:latest \
  python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'Device count: {torch.cuda.device_count()}')"
```

### Data Processing Pipelines

Accelerate data processing with GPU-enabled containers:

```bash
# Run RAPIDS for accelerated data science
docker run --gpus all -p 8888:8888 -p 8787:8787 -p 8786:8786 \
  rapidsai/rapidsai:cuda11.0-runtime-ubuntu18.04-py3.8

# Image processing with OpenCV CUDA support
docker run --gpus all -v $(pwd)/images:/images custom/opencv-cuda:latest \
  python process_images.py --input /images/input --output /images/processed
```

### GPU-Accelerated Web Services

Deploy containers that leverage GPUs for web application acceleration:

```bash
# Deploy GPU-accelerated inference API
docker run --gpus all -p 8000:8000 -v $(pwd)/models:/models \
  custom/inference-service:latest --model /models/model.pt --workers 4

# Run GPU-accelerated video transcoding service
docker run --gpus 'device=0,capabilities=compute,video' -p 3000:3000 -v $(pwd)/videos:/videos \
  jrottenberg/ffmpeg:nvidia -i /videos/input.mp4 -c:v h264_nvenc -preset slow /videos/output.mp4
```

## Performance Optimization

### GPU Memory Management

Optimize memory allocation for maximum GPU utilization:

```bash
# Tensorflow with memory growth enabled
docker run --gpus all tensorflow/tensorflow:latest-gpu python -c '
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices("GPU")
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
print("Memory growth enabled for", len(gpus), "GPUs")
'

# PyTorch with specific memory allocation
docker run --gpus all pytorch/pytorch:latest python -c '
import torch
torch.cuda.set_per_process_memory_fraction(0.8)
print(f"GPU memory reserved: 80% of {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
'
```

### Multi-GPU Scaling Strategies

Configure containers for optimal performance across multiple GPUs:

1. **Data Parallelism**: Duplicate model across GPUs, process different data batches
2. **Model Parallelism**: Split model layers across GPUs for large models
3. **Pipeline Parallelism**: Process different model stages across GPUs
4. **Hybrid Approaches**: Combine strategies for optimal resource utilization

```bash
# TensorFlow distributed training with data parallelism
docker run --gpus all tensorflow/tensorflow:latest-gpu python -c '
import tensorflow as tf
strategy = tf.distribute.MirroredStrategy()
print("Number of devices:", strategy.num_replicas_in_sync)
with strategy.scope():
    model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
    model.compile(loss="mse", optimizer="sgd")
print("Model compiled with distribution strategy")
'
```

### GPU-Specific Dockerfiles

Create optimized Dockerfiles for GPU workloads:

```dockerfile
FROM nvidia/cuda:11.6.2-cudnn8-devel-ubuntu20.04

# Install dependencies with versions matched to CUDA
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3-pip python3-dev \
    && rm -rf /var/lib/apt/lists/*

# Set up environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    TF_FORCE_GPU_ALLOW_GROWTH=true \
    CUDA_DEVICE_ORDER=PCI_BUS_ID

# Install framework with GPU support
RUN pip3 install --no-cache-dir tensorflow==2.9.0

# Create non-root user and set permissions
RUN useradd -m appuser
USER appuser
WORKDIR /app

# Copy application code
COPY --chown=appuser:appuser . /app/

# Verify GPU configuration
RUN python3 -c "import tensorflow as tf; print('TensorFlow version:', tf.__version__); print('GPU Available:', tf.test.is_gpu_available())"

# Command with optimized GPU flags
CMD ["python3", "train.py", "--use-mixed-precision", "--xla-acceleration"]
```

## Troubleshooting GPU Containers

### Common GPU Issues

Diagnose and resolve frequent GPU container problems:

1. **Driver compatibility**: Ensure container CUDA version matches host driver
2. **Memory allocation failures**: Address out-of-memory conditions
3. **Device visibility**: Troubleshoot GPU detection issues
4. **Performance degradation**: Identify bottlenecks and optimization opportunities

### Diagnostic Commands

Use these commands to troubleshoot GPU container issues:

```bash
# Check GPU visibility within container
docker run --gpus all nvidia/cuda:latest nvidia-smi

# Verify CUDA installation and version
docker run --gpus all nvidia/cuda:latest nvcc --version

# Test GPU compute capability
docker run --gpus all nvidia/cuda:latest cuda-samples/deviceQuery

# Debug GPU memory usage
docker run --gpus all nvidia/cuda:latest nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

### Driver Compatibility Matrix

::alert{type="info" class="mb-4"}
Ensure compatibility between container CUDA versions and host drivers for optimal performance.
::

::div{class="overflow-x-auto"}
  ::table{class="w-full bg-white dark:bg-gray-800 border border-gray-200 dark:border-gray-700 rounded-lg overflow-hidden"}
    ::thead{class="bg-gray-100 dark:bg-gray-700"}
      ::tr{class="border-b border-gray-200 dark:border-gray-700"}
        ::th{class="px-4 py-3 text-left font-semibold text-gray-700 dark:text-gray-300"}
          CUDA Version
        ::
        ::th{class="px-4 py-3 text-left font-semibold text-gray-700 dark:text-gray-300"}
          Minimum Driver Version (Linux)
        ::
        ::th{class="px-4 py-3 text-left font-semibold text-gray-700 dark:text-gray-300"}
          Minimum Driver Version (Windows)
        ::
      ::
    ::
    ::tbody
      ::tr{class="border-b border-gray-200 dark:border-gray-700 hover:bg-gray-50 dark:hover:bg-gray-700/30"}
        ::td{class="px-4 py-3 text-gray-800 dark:text-gray-300 font-medium"}
          CUDA 11.8
        ::
        ::td{class="px-4 py-3 text-gray-600 dark:text-gray-400"}
          520.61.05
        ::
        ::td{class="px-4 py-3 text-gray-600 dark:text-gray-400"}
          522.06
        ::
      ::
      ::tr{class="border-b border-gray-200 dark:border-gray-700 hover:bg-gray-50 dark:hover:bg-gray-700/30"}
        ::td{class="px-4 py-3 text-gray-800 dark:text-gray-300 font-medium"}
          CUDA 11.7
        ::
        ::td{class="px-4 py-3 text-gray-600 dark:text-gray-400"}
          515.43.04
        ::
        ::td{class="px-4 py-3 text-gray-600 dark:text-gray-400"}
          516.31
        ::
      ::
      ::tr{class="border-b border-gray-200 dark:border-gray-700 hover:bg-gray-50 dark:hover:bg-gray-700/30"}
        ::td{class="px-4 py-3 text-gray-800 dark:text-gray-300 font-medium"}
          CUDA 11.6
        ::
        ::td{class="px-4 py-3 text-gray-600 dark:text-gray-400"}
          510.39.01
        ::
        ::td{class="px-4 py-3 text-gray-600 dark:text-gray-400"}
          511.65
        ::
      ::
      ::tr{class="border-b border-gray-200 dark:border-gray-700 hover:bg-gray-50 dark:hover:bg-gray-700/30"}
        ::td{class="px-4 py-3 text-gray-800 dark:text-gray-300 font-medium"}
          CUDA 11.5
        ::
        ::td{class="px-4 py-3 text-gray-600 dark:text-gray-400"}
          495.29.05
        ::
        ::td{class="px-4 py-3 text-gray-600 dark:text-gray-400"}
          496.13
        ::
      ::
      ::tr{class="border-b border-gray-200 dark:border-gray-700 hover:bg-gray-50 dark:hover:bg-gray-700/30"}
        ::td{class="px-4 py-3 text-gray-800 dark:text-gray-300 font-medium"}
          CUDA 11.4
        ::
        ::td{class="px-4 py-3 text-gray-600 dark:text-gray-400"}
          470.57.02
        ::
        ::td{class="px-4 py-3 text-gray-600 dark:text-gray-400"}
          471.41
        ::
      ::
    ::
  ::
::

## Security Considerations

### GPU Isolation

Implement proper security measures for GPU-accelerated containers:

1. **Resource limitations**: Prevent resource exhaustion attacks
2. **User namespace isolation**: Run GPU workloads with non-root users
3. **Device access controls**: Restrict GPU capabilities to necessary functions
4. **Memory protection**: Prevent cross-container memory access

### Secure GPU Container Deployment

Follow these security best practices:

```bash
# Run with minimal GPU capabilities
docker run --gpus 'device=0,capabilities=compute,utility' --read-only --security-opt=no-new-privileges \
  --cap-drop=ALL -u 1000:1000 secure/gpu-app:latest

# Implement proper resource limits
docker run --gpus 1 --memory=4g --cpu-shares=1024 --pids-limit=100 secure/gpu-app:latest

# Use temporary filesystem for sensitive data
docker run --gpus all --tmpfs /tmp:rw,noexec,nosuid secure/gpu-app:latest
```

## Production Deployment Considerations

### Kubernetes Integration

Deploy GPU workloads in Kubernetes environments:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
  - name: gpu-container
    image: nvidia/cuda:11.6.2-base-ubuntu20.04
    command: ["nvidia-smi"]
    resources:
      limits:
        nvidia.com/gpu: 2  # Request 2 GPUs
    securityContext:
      runAsUser: 1000      # Run as non-root
      runAsGroup: 1000
      allowPrivilegeEscalation: false
  nodeSelector:
    accelerator: nvidia-tesla-a100  # Target specific GPU type
```

### Monitoring and Logging

Implement comprehensive GPU monitoring for production environments:

1. **Resource utilization**: Track GPU memory, compute, and power usage
2. **Error detection**: Monitor for hardware errors and driver issues
3. **Performance metrics**: Collect throughput and latency statistics
4. **Thermal monitoring**: Prevent overheating and thermal throttling

```bash
# Deploy Prometheus with NVIDIA DCGM exporter
docker run -d --gpus all --restart always \
  -p 9400:9400 \
  --name dcgm-exporter \
  nvidia/dcgm-exporter:latest

# Run Grafana for GPU metrics visualization
docker run -d -p 3000:3000 \
  --name grafana \
  -v grafana-storage:/var/lib/grafana \
  grafana/grafana:latest
```

## Best Practices and Recommendations

::div{class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-8"}
  ::div{class="bg-gradient-to-br from-blue-50 to-indigo-50 dark:from-blue-900/20 dark:to-indigo-900/20 rounded-xl p-6 shadow-sm"}
    ::div{class="flex items-center mb-4"}
      ::div{class="text-blue-600 dark:text-blue-400"}
        <Icon name="lucide:zap" class="h-6 w-6" />
      ::
      ::h3{class="ml-2 text-xl font-semibold"}
        GPU Workload Optimization
      ::
    ::
    ::div{class="space-y-3"}
      ::div{class="flex"}
        ::div{class="flex-shrink-0 h-6 w-6 flex items-center justify-center rounded-full bg-blue-100 dark:bg-blue-800 text-blue-600 dark:text-blue-300 font-bold text-sm mr-3"}
          1
        ::
        ::div{class="text-gray-700 dark:text-gray-300"}
          **Right-size containers**: Match container resources to workload requirements
        ::
      ::
      ::div{class="flex"}
        ::div{class="flex-shrink-0 h-6 w-6 flex items-center justify-center rounded-full bg-blue-100 dark:bg-blue-800 text-blue-600 dark:text-blue-300 font-bold text-sm mr-3"}
          2
        ::
        ::div{class="text-gray-700 dark:text-gray-300"}
          **Optimize data pipelines**: Minimize CPU-GPU data transfer bottlenecks
        ::
      ::
      ::div{class="flex"}
        ::div{class="flex-shrink-0 h-6 w-6 flex items-center justify-center rounded-full bg-blue-100 dark:bg-blue-800 text-blue-600 dark:text-blue-300 font-bold text-sm mr-3"}
          3
        ::
        ::div{class="text-gray-700 dark:text-gray-300"}
          **Implement mixed precision**: Use FP16/BF16 where appropriate for faster computation
        ::
      ::
      ::div{class="flex"}
        ::div{class="flex-shrink-0 h-6 w-6 flex items-center justify-center rounded-full bg-blue-100 dark:bg-blue-800 text-blue-600 dark:text-blue-300 font-bold text-sm mr-3"}
          4
        ::
        ::div{class="text-gray-700 dark:text-gray-300"}
          **Batch processing**: Optimize batch sizes for maximum GPU utilization
        ::
      ::
      ::div{class="flex"}
        ::div{class="flex-shrink-0 h-6 w-6 flex items-center justify-center rounded-full bg-blue-100 dark:bg-blue-800 text-blue-600 dark:text-blue-300 font-bold text-sm mr-3"}
          5
        ::
        ::div{class="text-gray-700 dark:text-gray-300"}
          **Pre-compile kernels**: Reduce runtime compilation overhead
        ::
      ::
    ::
  ::
  
  ::div{class="bg-gradient-to-br from-purple-50 to-pink-50 dark:from-purple-900/20 dark:to-pink-900/20 rounded-xl p-6 shadow-sm"}
    ::div{class="flex items-center mb-4"}
      ::div{class="text-purple-600 dark:text-purple-400"}
        <Icon name="lucide:layers" class="h-6 w-6" />
      ::
      ::h3{class="ml-2 text-xl font-semibold"}
        Implementation Strategy
      ::
    ::
    ::div{class="space-y-3"}
      ::div{class="flex"}
        ::div{class="flex-shrink-0 h-6 w-6 flex items-center justify-center rounded-full bg-purple-100 dark:bg-purple-800 text-purple-600 dark:text-purple-300 font-bold text-sm mr-3"}
          1
        ::
        ::div{class="text-gray-700 dark:text-gray-300"}
          **Assessment**: Identify workloads that benefit from GPU acceleration
        ::
      ::
      ::div{class="flex"}
        ::div{class="flex-shrink-0 h-6 w-6 flex items-center justify-center rounded-full bg-purple-100 dark:bg-purple-800 text-purple-600 dark:text-purple-300 font-bold text-sm mr-3"}
          2
        ::
        ::div{class="text-gray-700 dark:text-gray-300"}
          **Pilot deployment**: Test with non-critical workloads
        ::
      ::
      ::div{class="flex"}
        ::div{class="flex-shrink-0 h-6 w-6 flex items-center justify-center rounded-full bg-purple-100 dark:bg-purple-800 text-purple-600 dark:text-purple-300 font-bold text-sm mr-3"}
          3
        ::
        ::div{class="text-gray-700 dark:text-gray-300"}
          **Infrastructure planning**: Design for appropriate GPU distribution
        ::
      ::
      ::div{class="flex"}
        ::div{class="flex-shrink-0 h-6 w-6 flex items-center justify-center rounded-full bg-purple-100 dark:bg-purple-800 text-purple-600 dark:text-purple-300 font-bold text-sm mr-3"}
          4
        ::
        ::div{class="text-gray-700 dark:text-gray-300"}
          **Standardization**: Create repeatable patterns for GPU container deployment
        ::
      ::
      ::div{class="flex"}
        ::div{class="flex-shrink-0 h-6 w-6 flex items-center justify-center rounded-full bg-purple-100 dark:bg-purple-800 text-purple-600 dark:text-purple-300 font-bold text-sm mr-3"}
          5
        ::
        ::div{class="text-gray-700 dark:text-gray-300"}
          **Monitoring framework**: Implement comprehensive visibility into GPU utilization
        ::
      ::
    ::
  ::
::

## Conclusion

::div{class="bg-gradient-to-r from-blue-50 to-indigo-50 dark:from-blue-900/20 dark:to-indigo-900/20 rounded-xl p-6 mb-6"}
  ::div{class="prose dark:prose-invert max-w-none"}
    Docker's GPU Acceleration Framework transforms how organizations leverage high-performance computing resources in containerized environments. By providing a unified interface across GPU vendors, flexible resource allocation, and robust monitoring capabilities, the framework enables everything from AI/ML workloads to scientific computing and video processing at scale.
  ::
::

::div{class="mt-6"}
  ::h3{class="text-xl font-semibold mb-4 flex items-center"}
    <Icon name="lucide:check-circle" class="h-6 w-6 text-green-500 mr-2" /> Key Advantages
  ::

  ::div{class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-4"}
    ::div{class="bg-white dark:bg-gray-800 p-4 rounded-lg border border-gray-200 dark:border-gray-700 shadow-sm"}
      ::div{class="flex items-center mb-2"}
        ::div{class="bg-green-100 dark:bg-green-800 p-2 rounded-md"}
          <Icon name="lucide:check" class="h-5 w-5 text-green-600 dark:text-green-400" />
        ::
        ::div{class="ml-2 font-semibold"}
          Simplified Deployment
        ::
      ::
      ::div{class="text-sm text-gray-600 dark:text-gray-400"}
        Consistent GPU access across development and production
      ::
    ::
    
    ::div{class="bg-white dark:bg-gray-800 p-4 rounded-lg border border-gray-200 dark:border-gray-700 shadow-sm"}
      ::div{class="flex items-center mb-2"}
        ::div{class="bg-green-100 dark:bg-green-800 p-2 rounded-md"}
          <Icon name="lucide:check" class="h-5 w-5 text-green-600 dark:text-green-400" />
        ::
        ::div{class="ml-2 font-semibold"}
          Efficient Resource Utilization
        ::
      ::
      ::div{class="text-sm text-gray-600 dark:text-gray-400"}
        Fine-grained allocation and isolation
      ::
    ::
    
    ::div{class="bg-white dark:bg-gray-800 p-4 rounded-lg border border-gray-200 dark:border-gray-700 shadow-sm"}
      ::div{class="flex items-center mb-2"}
        ::div{class="bg-green-100 dark:bg-green-800 p-2 rounded-md"}
          <Icon name="lucide:check" class="h-5 w-5 text-green-600 dark:text-green-400" />
        ::
        ::div{class="ml-2 font-semibold"}
          Workload Portability
        ::
      ::
      ::div{class="text-sm text-gray-600 dark:text-gray-400"}
        Vendor-agnostic approach for heterogeneous environments
      ::
    ::
    
    ::div{class="bg-white dark:bg-gray-800 p-4 rounded-lg border border-gray-200 dark:border-gray-700 shadow-sm"}
      ::div{class="flex items-center mb-2"}
        ::div{class="bg-green-100 dark:bg-green-800 p-2 rounded-md"}
          <Icon name="lucide:check" class="h-5 w-5 text-green-600 dark:text-green-400" />
        ::
        ::div{class="ml-2 font-semibold"}
          Performance Optimization
        ::
      ::
      ::div{class="text-sm text-gray-600 dark:text-gray-400"}
        Advanced resource management for maximum throughput
      ::
    ::
    
    ::div{class="bg-white dark:bg-gray-800 p-4 rounded-lg border border-gray-200 dark:border-gray-700 shadow-sm"}
      ::div{class="flex items-center mb-2"}
        ::div{class="bg-green-100 dark:bg-green-800 p-2 rounded-md"}
          <Icon name="lucide:check" class="h-5 w-5 text-green-600 dark:text-green-400" />
        ::
        ::div{class="ml-2 font-semibold"}
          Enterprise Readiness
        ::
      ::
      ::div{class="text-sm text-gray-600 dark:text-gray-400"}
        Production-grade security and monitoring integration
      ::
    ::
  ::
::

::alert{type="success" class="mt-8"}
As GPU acceleration becomes increasingly critical for modern applications, Docker's comprehensive framework provides the foundation for scalable, secure, and high-performance containerized computing.
::